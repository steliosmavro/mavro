---
title: "Parallel Computing in C: Building a High-Performance K-NN Classifier"
date: "2024-09-12"
summary: "Deep dive into parallel programming with Pthreads, exploring how to achieve 8x speedup on multi-core systems. Learn thread management, synchronization, and optimization techniques through a real-world K-NN implementation."
tags: ["Performance", "C Programming", "Parallel Computing", "Algorithms"]
---

Parallel computing isn't just about throwing more cores at a problem. It's about understanding data dependencies, minimizing synchronization overhead, and architecting algorithms that scale. Let me share insights from implementing a parallel K-Nearest Neighbors classifier that achieved 8x speedup on 8-core systems.

## The Challenge

K-NN is embarrassingly parallel in theory - each point's classification is independent. But efficient implementation requires careful consideration of:
- Memory access patterns
- Thread synchronization
- Load balancing
- Cache coherence

## Architecture Overview

```c
typedef struct {
    int thread_id;
    int start_idx;
    int end_idx;
    double* distances;
    Point* training_data;
    Point* test_data;
    int* predictions;
} ThreadData;

void* classify_range(void* arg) {
    ThreadData* data = (ThreadData*)arg;
    
    for (int i = data->start_idx; i < data->end_idx; i++) {
        data->predictions[i] = knn_classify(
            &data->test_data[i],
            data->training_data,
            data->distances
        );
    }
    
    return NULL;
}
```

## Key Optimization Strategies

### 1. Data Partitioning

Static partitioning works well for uniform workloads:

```c
int chunk_size = num_test_points / num_threads;
for (int i = 0; i < num_threads; i++) {
    thread_data[i].start_idx = i * chunk_size;
    thread_data[i].end_idx = (i == num_threads - 1) ? 
        num_test_points : (i + 1) * chunk_size;
}
```

### 2. Memory Layout Optimization

Optimize for cache locality:

```c
// Bad: Array of structures (AoS)
typedef struct {
    double x, y, z;
    int label;
} Point_AoS;

// Good: Structure of arrays (SoA)
typedef struct {
    double* x;
    double* y; 
    double* z;
    int* labels;
} Points_SoA;

// SoA improves vectorization and cache usage
double euclidean_distance_vectorized(Points_SoA* points, int idx1, int idx2) {
    double dx = points->x[idx1] - points->x[idx2];
    double dy = points->y[idx1] - points->y[idx2];
    double dz = points->z[idx1] - points->z[idx2];
    return sqrt(dx*dx + dy*dy + dz*dz);
}
```

### 3. Thread Pool Implementation

Avoid thread creation overhead:

```c
typedef struct {
    pthread_t* threads;
    TaskQueue* queue;
    pthread_mutex_t mutex;
    pthread_cond_t condition;
    int shutdown;
} ThreadPool;

void* worker_thread(void* arg) {
    ThreadPool* pool = (ThreadPool*)arg;
    
    while (1) {
        pthread_mutex_lock(&pool->mutex);
        
        while (pool->queue->size == 0 && !pool->shutdown) {
            pthread_cond_wait(&pool->condition, &pool->mutex);
        }
        
        if (pool->shutdown) {
            pthread_mutex_unlock(&pool->mutex);
            break;
        }
        
        Task task = dequeue(pool->queue);
        pthread_mutex_unlock(&pool->mutex);
        
        task.function(task.arg);
    }
    
    return NULL;
}
```

### 4. NUMA-Aware Memory Allocation

For multi-socket systems:

```c
#include <numa.h>

void* numa_aware_alloc(size_t size, int node) {
    if (numa_available() < 0) {
        return malloc(size);
    }
    
    return numa_alloc_onnode(size, node);
}

// Distribute data across NUMA nodes
void distribute_data(Point* data, int num_points) {
    int num_nodes = numa_num_configured_nodes();
    int points_per_node = num_points / num_nodes;
    
    for (int node = 0; node < num_nodes; node++) {
        // Pin thread to CPU on this node
        numa_run_on_node(node);
        
        // Allocate and initialize data on this node
        // This ensures data locality
    }
}
```

### 5. Lock-Free Data Structures

For shared result aggregation:

```c
typedef struct {
    atomic_int* class_counts;
    int num_classes;
} AtomicHistogram;

void update_histogram_lockfree(AtomicHistogram* hist, int class_id) {
    atomic_fetch_add(&hist->class_counts[class_id], 1);
}

int get_majority_class(AtomicHistogram* hist) {
    int max_count = 0;
    int majority_class = 0;
    
    for (int i = 0; i < hist->num_classes; i++) {
        int count = atomic_load(&hist->class_counts[i]);
        if (count > max_count) {
            max_count = count;
            majority_class = i;
        }
    }
    
    return majority_class;
}
```

## Performance Analysis

### Speedup Results

```
Threads | Execution Time | Speedup | Efficiency
--------|----------------|---------|------------
1       | 10.24s        | 1.00x   | 100%
2       | 5.21s         | 1.97x   | 98.5%
4       | 2.68s         | 3.82x   | 95.5%
8       | 1.31s         | 7.82x   | 97.7%
16      | 0.98s         | 10.45x  | 65.3%
```

### Profiling Insights

Using `perf` and Intel VTune:

```bash
# Cache misses analysis
perf stat -e cache-misses,cache-references ./parallel_knn

# Thread synchronization overhead
perf record -e sched:sched_switch ./parallel_knn
perf report

# NUMA effects
numastat -c parallel_knn
```

## Lessons Learned

1. **Memory Bandwidth is Often the Bottleneck**: Even with perfect parallelization, memory access patterns matter more than core count.

2. **False Sharing Kills Performance**: Ensure thread-local data is cache-line aligned:
   ```c
   typedef struct {
       double result;
       char padding[64 - sizeof(double)]; // Cache line padding
   } AlignedResult;
   ```

3. **Dynamic Load Balancing**: For non-uniform workloads, work-stealing outperforms static partitioning.

4. **Measure, Don't Assume**: What works on one architecture may fail on another. Always profile.

## Advanced Techniques

### SIMD Integration

Combine thread-level and data-level parallelism:

```c
#include <immintrin.h>

void distance_batch_avx2(float* dist, float* a, float* b, int n) {
    for (int i = 0; i < n; i += 8) {
        __m256 va = _mm256_load_ps(&a[i]);
        __m256 vb = _mm256_load_ps(&b[i]);
        __m256 diff = _mm256_sub_ps(va, vb);
        __m256 sq = _mm256_mul_ps(diff, diff);
        _mm256_store_ps(&dist[i], sq);
    }
}
```

### GPU Offloading

For massive datasets:

```c
// OpenMP target offloading
#pragma omp target teams distribute parallel for
for (int i = 0; i < num_points; i++) {
    classify_point(i);
}
```

## Conclusion

Parallel programming in C requires deep understanding of both hardware and algorithms. The 8x speedup achieved in this K-NN implementation came from:
- Careful data partitioning
- Cache-aware memory layouts
- Minimal synchronization overhead
- Architecture-specific optimizations

The key is to start simple, measure everything, and optimize based on actual bottlenecks rather than assumptions.